{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x = pd.read_csv('/home/m/dev/ai/scaleout/trainingdata/train.csv', encoding='latin-1')\n",
    " \n",
    "x.drop(columns=['query', 'date', '0', 'user', 'index'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>1599994</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1599995</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1599996</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1599997</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1599998</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "0              0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1              1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2              2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3              3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4              4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...          ... ..         ...                           ...       ...   \n",
       "1599994  1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "0          scotthamilton   \n",
       "1               mattycus   \n",
       "2                ElleCTF   \n",
       "3                 Karoli   \n",
       "4               joy_wolf   \n",
       "...                  ...   \n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0        is upset that he can't update his Facebook by ...                                                                   \n",
       "1        @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2          my whole body feels itchy and like its on fire                                                                    \n",
       "3        @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                            @Kwesidei not the whole crew                                                                    \n",
       "...                                                    ...                                                                   \n",
       "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   \n",
       "\n",
       "[1599999 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Calculate the number of parts\n",
    "num_parts = len(x) // 10000\n",
    "remainder = len(x) % 10000\n",
    "\n",
    "# Create the 'x' directory if it doesn't exist\n",
    "os.makedirs('x', exist_ok=True)\n",
    "\n",
    "# Split the dataset into parts of 10,000 entries and write to files\n",
    "for i in range(num_parts):\n",
    "    part = x[i*10000:(i+1)*10000]\n",
    "    part.to_csv(f'x/part_{i+1}.csv', index=False)\n",
    "\n",
    "# Add the remainder as the last part if it exists\n",
    "if remainder > 0:\n",
    "    last_part = x[num_parts*10000:]\n",
    "    last_part.to_csv(f'x/part_{num_parts+1}.csv', index=False)\n",
    "\n",
    "print(f\"Number of parts: {num_parts + (1 if remainder > 0 else 0)}\")\n",
    "for i in range(num_parts + (1 if remainder > 0 else 0)):\n",
    "    file_path = f'x/part_{i+1}.csv'\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"Part {i+1} size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "email = pd.read_csv('/home/m/dev/scaleout/emails.csv', encoding='latin-1')\n",
    "\n",
    "email.drop(columns=['file'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517401/517401 [01:26<00:00, 5975.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Message-ID  \\\n",
      "0  <18782981.1075855378110.JavaMail.evans@thyme>   \n",
      "1  <15464986.1075855378456.JavaMail.evans@thyme>   \n",
      "2  <24216240.1075855687451.JavaMail.evans@thyme>   \n",
      "3  <13505866.1075863688222.JavaMail.evans@thyme>   \n",
      "4  <30922949.1075863688243.JavaMail.evans@thyme>   \n",
      "\n",
      "                                    Date                     From  \\\n",
      "0  Mon, 14 May 2001 16:39:00 -0700 (PDT)  phillip.allen@enron.com   \n",
      "1   Fri, 4 May 2001 13:51:00 -0700 (PDT)  phillip.allen@enron.com   \n",
      "2  Wed, 18 Oct 2000 03:00:00 -0700 (PDT)  phillip.allen@enron.com   \n",
      "3  Mon, 23 Oct 2000 06:13:00 -0700 (PDT)  phillip.allen@enron.com   \n",
      "4  Thu, 31 Aug 2000 05:07:00 -0700 (PDT)  phillip.allen@enron.com   \n",
      "\n",
      "                        To    Subject           X-From  \\\n",
      "0     tim.belden@enron.com             Phillip K Allen   \n",
      "1  john.lavorato@enron.com        Re:  Phillip K Allen   \n",
      "2   leah.arsdall@enron.com   Re: test  Phillip K Allen   \n",
      "3    randall.gay@enron.com             Phillip K Allen   \n",
      "4     greg.piper@enron.com  Re: Hello  Phillip K Allen   \n",
      "\n",
      "                                                X-To X-cc X-bcc  \\\n",
      "0           Tim Belden <Tim Belden/Enron@EnronXGate>              \n",
      "1  John J Lavorato <John J Lavorato/ENRON@enronXg...              \n",
      "2                                   Leah Van Arsdall              \n",
      "3                                      Randall L Gay              \n",
      "4                                         Greg Piper              \n",
      "\n",
      "                                            X-Folder X-Origin  \\\n",
      "0  \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...  Allen-P   \n",
      "1  \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...  Allen-P   \n",
      "2    \\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail  Allen-P   \n",
      "3    \\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail  Allen-P   \n",
      "4    \\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail  Allen-P   \n",
      "\n",
      "                    X-FileName  \\\n",
      "0  pallen (Non-Privileged).pst   \n",
      "1  pallen (Non-Privileged).pst   \n",
      "2                   pallen.nsf   \n",
      "3                   pallen.nsf   \n",
      "4                   pallen.nsf   \n",
      "\n",
      "                                             Content  \n",
      "0                               Here is our forecast  \n",
      "1  Traveling to have a business meeting takes the...  \n",
      "2                     test successful.  way to go!!!  \n",
      "3  Randy,\\n\\n Can you send me a schedule of the s...  \n",
      "4                  Let's shoot for Tuesday at 11:45.  \n",
      "Extracted data saved to 'extracted_emails.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_email(row):\n",
    "    extracted_data = {}\n",
    "    relevant_fields = ['Message-ID', 'Date', 'From', 'To', 'Subject', 'X-From', 'X-To', 'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName']\n",
    "    \n",
    "    # Extract header information\n",
    "    for line in row['message'].split('\\n'):\n",
    "        for field in relevant_fields:\n",
    "            if line.startswith(field + ':'):\n",
    "                extracted_data[field] = line.split(':', 1)[1].strip()\n",
    "    \n",
    "    # Extract the message content\n",
    "    content_start = row['message'].find('\\n\\n')  # Assuming content starts after a blank line\n",
    "    if content_start != -1:\n",
    "        extracted_data['Content'] = row['message'][content_start:].strip()\n",
    "    else:\n",
    "        extracted_data['Content'] = ''  # If no content is found\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "# Use all available cores\n",
    "num_cores = mp.cpu_count()\n",
    "\n",
    "# Create a pool of workers\n",
    "pool = mp.Pool(num_cores)\n",
    "\n",
    "# Process emails in parallel\n",
    "all_extracted_data = list(tqdm(pool.imap(process_email, [row for _, row in email.iterrows()]), total=len(email)))\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "extracted_df = pd.DataFrame(all_extracted_data)\n",
    "\n",
    "# Display the first few rows of the extracted data\n",
    "print(extracted_df.head())\n",
    "\n",
    "# Save the extracted data to a CSV file\n",
    "extracted_df.to_csv('extracted_emails.csv', index=False)\n",
    "print(\"Extracted data saved to 'extracted_emails.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 100%|██████████| 52/52 [00:12<00:00,  4.32it/s, Last saved=emails/part_51.csv]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the extracted data into 52 files of 10,000 entries each (or less for the last file).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extracted = pd.read_csv('extracted_emails.csv')\n",
    "extracted.drop(columns=['Message-ID', 'From', 'To', 'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName', 'Date'], inplace=True)\n",
    "extracted\n",
    "\n",
    "\n",
    "# Calculate the number of files needed\n",
    "num_files = len(extracted) // 10000 + (1 if len(extracted) % 10000 > 0 else 0)\n",
    "\n",
    "# Split the DataFrame and save to separate CSV files\n",
    "with tqdm(total=num_files, desc=\"Saving files\") as pbar:\n",
    "    for i in range(num_files):\n",
    "        start_idx = i * 10000\n",
    "        end_idx = min((i + 1) * 10000, len(extracted))\n",
    "        \n",
    "        # Create a subset of the DataFrame\n",
    "        subset = extracted.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Save the subset to a CSV file\n",
    "        filename = f'emails/part_{i}.csv'\n",
    "        subset.to_csv(filename, index=False)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\"Last saved\": filename})\n",
    "\n",
    "print(f\"Split the extracted data into {num_files} files of 10,000 entries each (or less for the last file).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
